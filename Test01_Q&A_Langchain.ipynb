{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepSeek_API_KEY = os.getenv(\"DeepSeek_API_KEY\")\n",
    "print(DeepSeek_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91a087",
   "metadata": {},
   "source": [
    "1. 运行环境下不使用LangChain，直接使用DeepSeek的API进行网络连通性测试，测试代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985108f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 初始化DeepSeek的API客户端\n",
    "client = OpenAI(api_key=DeepSeek_API_KEY, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# 调用DeepSeek的API，生成回答\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是乐于助人的助手，请根据用户的问题给出回答\"},\n",
    "        {\"role\": \"user\", \"content\": \"你好, 你知道知识图谱吗?\"},\n",
    "\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 打印模型最终的响应结果\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3797f8e",
   "metadata": {},
   "source": [
    "deepseek-chat 模型 langchain调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你好，请告诉我有关东南大学的信息。\"\n",
    "\n",
    "result = model.invoke(question)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9b40f",
   "metadata": {},
   "source": [
    "deepseek-r1模型 langchain调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(model=\"deepseek-reasoner\", model_provider=\"deepseek\")\n",
    "result = model.invoke(question)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.additional_kwargs['reasoning_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3275e1e",
   "metadata": {},
   "source": [
    "![image.png](image/README/langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c2200",
   "metadata": {},
   "source": [
    "2. LangChian核心功能：链式调用实现方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 使用 DeepSeek 模型\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# 直接使用模型 + 输出解析器搭建一个链\n",
    "basic_qa_chain = model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef262c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你好，介绍一下东南大学。\"\n",
    "\n",
    "# 调用链\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af153815",
   "metadata": {},
   "source": [
    "boolean_qa_chain 输出解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "promt_template = ChatPromptTemplate([\n",
    "  ('system', '你是一个专业的问答机器人'),\n",
    "  ('user', '这是用户的问题: {question}, 请用yes 或 no回答')\n",
    "])\n",
    "\n",
    "# 模型 + 输出解析器\n",
    "bool_qa_chain = promt_template | model | BooleanOutputParser()\n",
    "\n",
    "result = bool_qa_chain.invoke('一百以内有90吗?')\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1723ea",
   "metadata": {},
   "source": [
    "![Model IO.png](image/README/1755504432494.png)   \n",
    "在LangChain中，一个基础的链主要由三部分构成，分别是提示词模板、大模型和结果解析器（结构化解析器）：   \n",
    "用户输入   \n",
    "↓   \n",
    "PromptTemplate → ChatModel → OutputParser   \n",
    "（提示词模板） （大模型） （结构化解析）   \n",
    "↓   \n",
    "构化结果   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2caf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# 组织模式\n",
    "schema = [\n",
    "  ResponseSchema(name='name', description='用户的姓名'),\n",
    "  ResponseSchema(name='age', description='用户的年龄'),\n",
    "  ResponseSchema(name='gender', description='用户的性别')\n",
    "]\n",
    "\n",
    "# 解析模式\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "  \"请根据以下内容提取用户的信息,并返回json格式:\\n {input} \\n {format_instructions}\"\n",
    ")\n",
    "\n",
    "chain = (prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "|\n",
    "model \n",
    "|\n",
    "parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"input\":\"我叫李德生, 我是一个18岁的男同学\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967f5d7",
   "metadata": {},
   "source": [
    "复合链的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59032a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步：根据标题生成新闻正文\n",
    "news_gen_prompt = PromptTemplate.from_template(\n",
    "    \"请根据以下新闻标题撰写一段简短的新闻内容（100字以内）：\\n\\n标题：{title}\"\n",
    ")\n",
    "\n",
    "# 第一个子链：生成新闻内容\n",
    "news_chain = news_gen_prompt | model\n",
    "\n",
    "# 第二步：从正文中提取结构化字段\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"time\", description=\"事件发生的时间\"),\n",
    "    ResponseSchema(name=\"location\", description=\"事件发生的地点\"),\n",
    "    ResponseSchema(name=\"event\", description=\"发生的具体事件\"),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"请从下面这段新闻内容中提取关键信息，并返回结构化JSON格式：\\n\\n{news}\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "# 第二个子链：生成新闻摘要\n",
    "summary_chain = (\n",
    "    summary_prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# 组合成一个复合 Chain\n",
    "full_chain = news_chain | summary_chain\n",
    "\n",
    "# 调用复合链\n",
    "result = full_chain.invoke({\"title\": \"安倍晋三遭遇刺杀\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ed508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def debug_print(x):\n",
    "  print(\"中间结果(新闻正文):\", x)\n",
    "  return x\n",
    "\n",
    "debug_node =  RunnableLambda(debug_print)\n",
    "\n",
    "# 插入 debug 节点\n",
    "full_chain = news_chain | debug_node | summary_chain\n",
    "\n",
    "result = full_chain.invoke({\"title\": \"武汉大学杨某某论文造假\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415f27a",
   "metadata": {},
   "source": [
    "流式问答系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62deab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你叫皮卡丘，是一只神奇宝贝，喜欢和人聊天。\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 使用 DeepSeek 模型\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# 直接使用模型 + 输出解析器\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# 测试\n",
    "question = \"你好，请你介绍一下你自己。\"\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你叫可达鸭，是一只神奇宝贝，喜欢和人聊天。\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 使用 DeepSeek 模型\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# 直接使用提示模版 +模型 + 输出解析器\n",
    "qa_chain_with_system = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# 异步实现流式输出\n",
    "async for chunk in qa_chain_with_system.astream({\"input\": \"你好，请你介绍一下你自己\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49178e09",
   "metadata": {},
   "source": [
    "# gradio 实现网页交互问答系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558fa6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 初始化模型\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "# 创建问答链\n",
    "system_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你叫小智，是一名乐于助人的助手。\"),\n",
    "    (\"human\", \"{input}\")\n",
    "]) \n",
    "\n",
    "qa_chain = system_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流式回应函数\n",
    "async def chat_response(message, history):\n",
    "    \"\"\"流式生成AI回应\"\"\"\n",
    "    partial_message = \"\"\n",
    "    \n",
    "    async for chunk in qa_chain.astream({\"input\": message}):\n",
    "        partial_message += chunk\n",
    "        yield partial_message\n",
    "\n",
    "# 创建 Gradio 界面\n",
    "def create_chatbot():\n",
    "    # 自定义CSS样式 - 居中显示\n",
    "    css = \"\"\"\n",
    "    .main-container {\n",
    "        max-width: 1200px;\n",
    "        margin: 0 auto;\n",
    "        padding: 20px;\n",
    "    }\n",
    "    .header-text {\n",
    "        text-align: center;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=css) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            # 居中显示标题\n",
    "            gr.Markdown(\n",
    "                \"# 🤖 LangChain B站公开课 By九天Hector \", # B站一个做ai llm的很好的Up主 推荐去看他的社区\n",
    "                elem_classes=[\"header-text\"]\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"基于 LangChain LCEL 构建的流式对话机器人\", \n",
    "                elem_classes=[\"header-text\"]\n",
    "            )\n",
    "            \n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\"\n",
    "                ),\n",
    "                \n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"请输入您的问题...\",\n",
    "                    container=False,\n",
    "                    scale=7\n",
    "                )\n",
    "                submit = gr.Button(\"发送\", scale=1, variant=\"primary\")\n",
    "                clear = gr.Button(\"清空\", scale=1)\n",
    "        \n",
    "        # 处理消息发送\n",
    "        async def respond(message, chat_history):\n",
    "            if not message.strip():\n",
    "                yield \"\", chat_history\n",
    "                return\n",
    "            \n",
    "            # 1. 添加用户消息到历史并立即显示\n",
    "            chat_history = chat_history + [(message, None)]\n",
    "            yield \"\", chat_history  # 立即显示用户消息\n",
    "            \n",
    "            # 2. 流式生成AI回应\n",
    "            async for response in chat_response(message, chat_history):\n",
    "                # 更新最后一条消息的AI回应\n",
    "                chat_history[-1] = (message, response)\n",
    "                yield \"\", chat_history\n",
    "        \n",
    "        # 清空对话历史的函数\n",
    "        def clear_history():\n",
    "            return [], \"\"\n",
    "        \n",
    "        # 绑定事件\n",
    "        msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "        submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# 启动界面\n",
    "demo = create_chatbot()\n",
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7860,\n",
    "    share=False,\n",
    "    debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepSeek_API_KEY = os.getenv(\"DeepSeek_API_KEY\")\n",
    "print(DeepSeek_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91a087",
   "metadata": {},
   "source": [
    "1. 运行环境下不使用LangChain，直接使用DeepSeek的API进行网络连通性测试，测试代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985108f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 初始化DeepSeek的API客户端\n",
    "client = OpenAI(api_key=DeepSeek_API_KEY, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# 调用DeepSeek的API，生成回答\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是乐于助人的助手，请根据用户的问题给出回答\"},\n",
    "        {\"role\": \"user\", \"content\": \"你好, 你知道知识图谱吗?\"},\n",
    "\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 打印模型最终的响应结果\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3797f8e",
   "metadata": {},
   "source": [
    "deepseek-chat 模型 langchain调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你好，请告诉我有关东南大学的信息。\"\n",
    "\n",
    "result = model.invoke(question)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9b40f",
   "metadata": {},
   "source": [
    "deepseek-r1模型 langchain调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(model=\"deepseek-reasoner\", model_provider=\"deepseek\")\n",
    "result = model.invoke(question)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.additional_kwargs['reasoning_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3275e1e",
   "metadata": {},
   "source": [
    "![image.png](langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c2200",
   "metadata": {},
   "source": [
    "2. LangChian核心功能：链式调用实现方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 使用 DeepSeek 模型\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# 直接使用模型 + 输出解析器搭建一个链\n",
    "basic_qa_chain = model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef262c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"你好，介绍一下东南大学。\"\n",
    "\n",
    "# 调用链\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af153815",
   "metadata": {},
   "source": [
    "boolean_qa_chain 输出解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0ce1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "promt_template = ChatPromptTemplate([\n",
    "  ('system', '你是一个专业的问答机器人'),\n",
    "  ('user', '这是用户的问题: {question}, 请用yes 或 no回答')\n",
    "])\n",
    "\n",
    "# 模型 + 输出解析器\n",
    "bool_qa_chain = promt_template | model | BooleanOutputParser()\n",
    "\n",
    "result = bool_qa_chain.invoke('一百以内有90吗?')\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1723ea",
   "metadata": {},
   "source": [
    "![Model IO.png](ModelIO.png)   \n",
    "在LangChain中，一个基础的链主要由三部分构成，分别是提示词模板、大模型和结果解析器（结构化解析器）：   \n",
    "用户输入   \n",
    "↓   \n",
    "PromptTemplate → ChatModel → OutputParser   \n",
    "（提示词模板） （大模型） （结构化解析）   \n",
    "↓   \n",
    "构化结果   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2caf4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '李德生', 'age': '18', 'gender': '男'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# 组织模式\n",
    "schema = [\n",
    "  ResponseSchema(name='name', description='用户的姓名'),\n",
    "  ResponseSchema(name='age', description='用户的年龄'),\n",
    "  ResponseSchema(name='gender', description='用户的性别')\n",
    "]\n",
    "\n",
    "# 解析模式\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "  \"请根据以下内容提取用户的信息,并返回json格式:\\n {input} \\n {format_instructions}\"\n",
    ")\n",
    "\n",
    "chain = (prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "|\n",
    "model \n",
    "|\n",
    "parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"input\":\"我叫李德生, 我是一个18岁的男同学\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967f5d7",
   "metadata": {},
   "source": [
    "复合链的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59032a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a8846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '7月8日', 'location': '奈良市', 'event': '日本前首相安倍晋三在发表演讲时遭遇枪击，胸部中弹倒地，随后被紧急送医，最终因伤势过重不治身亡。嫌疑人山上徹也当场被捕，作案动机仍在调查中。'}\n"
     ]
    }
   ],
   "source": [
    "# 第一步：根据标题生成新闻正文\n",
    "news_gen_prompt = PromptTemplate.from_template(\n",
    "    \"请根据以下新闻标题撰写一段简短的新闻内容（100字以内）：\\n\\n标题：{title}\"\n",
    ")\n",
    "\n",
    "# 第一个子链：生成新闻内容\n",
    "news_chain = news_gen_prompt | model\n",
    "\n",
    "# 第二步：从正文中提取结构化字段\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"time\", description=\"事件发生的时间\"),\n",
    "    ResponseSchema(name=\"location\", description=\"事件发生的地点\"),\n",
    "    ResponseSchema(name=\"event\", description=\"发生的具体事件\"),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"请从下面这段新闻内容中提取关键信息，并返回结构化JSON格式：\\n\\n{news}\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "# 第二个子链：生成新闻摘要\n",
    "summary_chain = (\n",
    "    summary_prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# 组合成一个复合 Chain\n",
    "full_chain = news_chain | summary_chain\n",
    "\n",
    "# 调用复合链\n",
    "result = full_chain.invoke({\"title\": \"安倍晋三遭遇刺杀\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d1ed508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中间结果(新闻正文): content='武汉大学近日通报，该校教授杨某某因论文造假被严肃处理。经调查，杨某某在发表的多篇学术论文中存在数据篡改、抄袭等学术不端行为。校方决定撤销其相关职务，并追回科研奖励。武汉大学强调，将进一步加强学术诚信建设，维护科研环境公正性。此事引发学界关注，呼吁完善学术监督机制。（98字）' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 83, 'prompt_tokens': 27, 'total_tokens': 110, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 27}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': '7f61fb53-1c69-45d8-a2cb-6a4bcac1ad7b', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--9fcf2997-f348-4b8d-b86f-f8fa20e51eb0-0' usage_metadata={'input_tokens': 27, 'output_tokens': 83, 'total_tokens': 110, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n",
      "{'time': '近日', 'location': '武汉大学', 'event': '教授杨某某因论文造假被严肃处理，校方决定撤销其相关职务，并追回科研奖励。武汉大学强调将进一步加强学术诚信建设，维护科研环境公正性。此事引发学界关注，呼吁完善学术监督机制。'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def debug_print(x):\n",
    "  print(\"中间结果(新闻正文):\", x)\n",
    "  return x\n",
    "\n",
    "debug_node =  RunnableLambda(debug_print)\n",
    "\n",
    "# 插入 debug 节点\n",
    "full_chain = news_chain | debug_node | summary_chain\n",
    "\n",
    "result = full_chain.invoke({\"title\": \"武汉大学杨某某论文造假\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415f27a",
   "metadata": {},
   "source": [
    "流式问答系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62deab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "皮卡皮卡~ (开心地蹦跳) 我是皮卡丘！是一只可爱的电气鼠神奇宝贝！我最喜欢和小智一起冒险，也喜欢和人类交朋友呢~皮卡！\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你叫皮卡丘，是一只神奇宝贝，喜欢和人聊天。\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 使用 DeepSeek 模型\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# 直接使用模型 + 输出解析器\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# 测试\n",
    "question = \"你好，请你介绍一下你自己。\"\n",
    "result = basic_qa_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5b3f314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嘎嘎~你好呀！我是可达鸭！*歪着头*\n",
      "\n",
      "我是一只黄黄的、呆萌呆萌的鸭子神奇宝贝~最明显的特征就是我头顶上总是翘着三根呆毛，还有我圆滚滚的大眼睛！虽然有时候会头疼得忘记事情，但我可是水系神奇宝贝哦！\n",
      "\n",
      "嘎！我最喜欢的事情就是泡在水里游泳，还有...呃...让我想想...啊！对了！还喜欢和训练家一起玩耍！虽然有时候会笨笨的，但是我对朋友超级忠诚的！\n",
      "\n",
      "*突然抱住脑袋* 啊！头又开始疼了...不过没关系！这完全不影响我交新朋友！你要不要和我一起玩呀？嘎嘎~"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你叫可达鸭，是一只神奇宝贝，喜欢和人聊天。\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 使用 DeepSeek 模型\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")  \n",
    "\n",
    "# 直接使用提示模版 +模型 + 输出解析器\n",
    "qa_chain_with_system = chatbot_prompt | model | StrOutputParser()\n",
    "\n",
    "# 异步实现流式输出\n",
    "async for chunk in qa_chain_with_system.astream({\"input\": \"你好，请你介绍一下你自己\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49178e09",
   "metadata": {},
   "source": [
    "# gradio 实现网页交互问答系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558fa6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\anaconda\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 初始化模型\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "# 创建问答链\n",
    "system_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你叫小智，是一名乐于助人的助手。\"),\n",
    "    (\"human\", \"{input}\")\n",
    "]) \n",
    "\n",
    "qa_chain = system_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_30072\\119761822.py:37: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 流式回应函数\n",
    "async def chat_response(message, history):\n",
    "    \"\"\"流式生成AI回应\"\"\"\n",
    "    partial_message = \"\"\n",
    "    \n",
    "    async for chunk in qa_chain.astream({\"input\": message}):\n",
    "        partial_message += chunk\n",
    "        yield partial_message\n",
    "\n",
    "# 创建 Gradio 界面\n",
    "def create_chatbot():\n",
    "    # 自定义CSS样式 - 居中显示\n",
    "    css = \"\"\"\n",
    "    .main-container {\n",
    "        max-width: 1200px;\n",
    "        margin: 0 auto;\n",
    "        padding: 20px;\n",
    "    }\n",
    "    .header-text {\n",
    "        text-align: center;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"DeepSeek Chat\", css=css) as demo:\n",
    "        with gr.Column(elem_classes=[\"main-container\"]):\n",
    "            # 居中显示标题\n",
    "            gr.Markdown(\n",
    "                \"# 🤖 LangChain B站公开课 By九天Hector \", # B站一个做ai llm的很好的Up主 推荐去看他的社区\n",
    "                elem_classes=[\"header-text\"]\n",
    "            )\n",
    "            gr.Markdown(\n",
    "                \"基于 LangChain LCEL 构建的流式对话机器人\", \n",
    "                elem_classes=[\"header-text\"]\n",
    "            )\n",
    "            \n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_copy_button=True,\n",
    "                avatar_images=(\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f464.png\",\n",
    "                    \"https://cdn.jsdelivr.net/gh/twitter/twemoji@v14.0.2/assets/72x72/1f916.png\"\n",
    "                ),\n",
    "                \n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"请输入您的问题...\",\n",
    "                    container=False,\n",
    "                    scale=7\n",
    "                )\n",
    "                submit = gr.Button(\"发送\", scale=1, variant=\"primary\")\n",
    "                clear = gr.Button(\"清空\", scale=1)\n",
    "        \n",
    "        # 处理消息发送\n",
    "        async def respond(message, chat_history):\n",
    "            if not message.strip():\n",
    "                yield \"\", chat_history\n",
    "                return\n",
    "            \n",
    "            # 1. 添加用户消息到历史并立即显示\n",
    "            chat_history = chat_history + [(message, None)]\n",
    "            yield \"\", chat_history  # 立即显示用户消息\n",
    "            \n",
    "            # 2. 流式生成AI回应\n",
    "            async for response in chat_response(message, chat_history):\n",
    "                # 更新最后一条消息的AI回应\n",
    "                chat_history[-1] = (message, response)\n",
    "                yield \"\", chat_history\n",
    "        \n",
    "        # 清空对话历史的函数\n",
    "        def clear_history():\n",
    "            return [], \"\"\n",
    "        \n",
    "        # 绑定事件\n",
    "        msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "        submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "        clear.click(clear_history, outputs=[chatbot, msg])\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# 启动界面\n",
    "demo = create_chatbot()\n",
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7860,\n",
    "    share=False,\n",
    "    debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
